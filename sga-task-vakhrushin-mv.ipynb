{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# User routes on the site\n",
    "## Description\n",
    "**Clickstream** is a sequence of user actions on a website. It allows you to understand how users interact with the site. In this task, you need to find the most frequent custom routes.\n",
    "\n",
    "## Input data\n",
    "Input data is а table with clickstream data in file `hdfs:/data/clickstream.csv`.\n",
    "\n",
    "### Table structure\n",
    "* `user_id (int)` - Unique user identifier.\n",
    "* `session_id (int)` - Unique identifier for the user session. The user's session lasts until the identifier changes.\n",
    "* `event_type (string)` - Event type from the list:\n",
    "    * **page** - visit to the page\n",
    "    * **event** - any action on the page\n",
    "    * <b>&lt;custom&gt;</b> - string with any other type\n",
    "* `event_type (string)` - Page on the site.\n",
    "* `timestamp (int)` - Unix-timestamp of action.\n",
    "\n",
    "### Browser errors\n",
    "Errors can sometimes occur in the user's browser - after such an error appears, we can no longer trust the data of this session and all the following lines after the error or at the same time with it are considered corrupted and **should not be counted** in statistics.\n",
    "\n",
    "When an error occurs on the page, a random string containing the word **error** will be written to the `event_type` field.\n",
    "\n",
    "### Sample of user session\n",
    "<pre>\n",
    "+-------+----------+------------+----------+----------+\n",
    "|user_id|session_id|  event_type|event_page| timestamp|\n",
    "+-------+----------+------------+----------+----------+\n",
    "|    562|       507|        page|      main|1620494781|\n",
    "|    562|       507|       event|      main|1620494788|\n",
    "|    562|       507|       event|      main|1620494798|\n",
    "|    562|       507|        page|    family|1620494820|\n",
    "|    562|       507|       event|    family|1620494828|\n",
    "|    562|       507|        page|      main|1620494848|\n",
    "|    562|       507|wNaxLlerrorU|      main|1620494865|\n",
    "|    562|       507|       event|      main|1620494873|\n",
    "|    562|       507|        page|      news|1620494875|\n",
    "|    562|       507|        page|   tariffs|1620494876|\n",
    "|    562|       507|       event|   tariffs|1620494884|\n",
    "|    562|       514|        page|      main|1620728918|\n",
    "|    562|       514|       event|      main|1620729174|\n",
    "|    562|       514|        page|   archive|1620729674|\n",
    "|    562|       514|        page|     bonus|1620729797|\n",
    "|    562|       514|        page|   tariffs|1620731090|\n",
    "|    562|       514|       event|   tariffs|1620731187|\n",
    "+-------+----------+------------+----------+----------+\n",
    "</pre>\n",
    "\n",
    "#### Correct user routes for a given user:\n",
    "* **Session 507**: main-family-main\n",
    "* **Session 514**: main-archive-bonus-tariffs\n",
    "\n",
    "Route elements are ordered by the time they appear in the clickstream, from earliest to latest.\n",
    "\n",
    "The route must be accounted for completely before the end of the session or an error in the session.\n",
    "\n",
    "## Task\n",
    "You need to use the Spark SQL, Spark RDD and Spark DF interfaces to create a solution file, the lines of which contain **the 30 most frequent user routes** on the site.\n",
    "\n",
    "Each line of the file should contain the `route` and `count` values **separated by tabs**, where:\n",
    "* `route` - route on the site, consisting of pages separated by \"-\".\n",
    "* `count` - the number of user sessions in which this route was.\n",
    "\n",
    "The lines must be **ordered in descending order** of the `count` field.\n",
    "\n",
    "## Criteria\n",
    "You can get maximum of 3.5 points (final grade) for this assignment, depedning on the number of interface you manage to leverage. The criteria are as follows:\n",
    "\n",
    "* 0.5 points – Spark SQL solution with 1 query\n",
    "* 0.5 points – Spark SQL solution with <=2 queries\n",
    "* 0.5 points – Spark RDD solution\n",
    "* 0.5 points – Spark DF solution\n",
    "* 0.5 points – your solution algorithm is relatively optimized, i.e.: no O^2 or O^3 complexities; appropriate object usage; no data leaks etc. This is evaluated by staff.\n",
    "* 1 point – 1 on 1 screening session. During this session staff member can ask you questions regarding your solution logic, framework usage, questionable parts of your code etc. If your code is clean enough, the staff member can just ask you to solve a theoretical problem connected to Spark.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/21 20:15:31 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/01/21 20:15:31 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    }
   ],
   "source": [
    "ram_size = 6\n",
    "num_cores = 6\n",
    "spark = (\n",
    "    SparkSession.builder.master(f\"local[{num_cores}]\")\n",
    "    .config(\"spark.local.dir\", \"spark_tmp\")\n",
    "    .config(\"spark.driver.memory\", f\"{ram_size}g\")\n",
    "    .config(\"spark.driver.maxResultSize\", f\"{ram_size}g\")\n",
    "    .config(\"spark.storage.memoryFraction\", \"1\")\n",
    "    # Required for large column names.\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"2000\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\")\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\")\n",
    "    .config(\"spark.ui.port\", \"8082\")\n",
    "    # To opimize toPandas speed\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL solution with 1 query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"clickstream.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mikhailvakhrushin/opt/anaconda3/envs/testing/lib/python3.10/site-packages/pyspark/sql/dataframe.py:330: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(df)\n",
    "df.registerTempTable('clickstream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- session_id: long (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- event_page: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark SQL solution with <= 2 queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_single_query = \"\"\"\n",
    "WITH error_data AS (\n",
    "    SELECT\n",
    "        user_id,\n",
    "        session_id,\n",
    "        MIN(timestamp) AS error_time\n",
    "    FROM\n",
    "        clickstream\n",
    "    WHERE\n",
    "        event_type LIKE '%error%'\n",
    "    GROUP BY\n",
    "        user_id,\n",
    "        session_id\n",
    "),\n",
    "unique_sessions AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        CONCAT(user_id, '_', session_id) AS unique_id\n",
    "    FROM\n",
    "        clickstream\n",
    "),\n",
    "ordered_sessions AS (\n",
    "    SELECT\n",
    "        *,\n",
    "        ROW_NUMBER() OVER (PARTITION BY unique_id ORDER BY timestamp) AS row_num\n",
    "    FROM\n",
    "        unique_sessions\n",
    "),\n",
    "filtered_sessions AS (\n",
    "    SELECT\n",
    "        u.*,\n",
    "        e.error_time\n",
    "    FROM\n",
    "        ordered_sessions u\n",
    "    LEFT JOIN\n",
    "        error_data e ON u.user_id = e.user_id AND u.session_id = e.session_id\n",
    "    WHERE\n",
    "        e.session_id IS NULL OR u.timestamp < e.error_time\n",
    "),\n",
    "distinct_pages AS (\n",
    "    SELECT\n",
    "        unique_id,\n",
    "        event_page,\n",
    "        LAG(event_page, 1) OVER (PARTITION BY unique_id ORDER BY timestamp) AS prev_page\n",
    "    FROM\n",
    "        filtered_sessions\n",
    "),\n",
    "no_duplicates AS (\n",
    "    SELECT\n",
    "        unique_id,\n",
    "        event_page\n",
    "    FROM\n",
    "        distinct_pages\n",
    "    WHERE\n",
    "        (prev_page IS NULL OR event_page <> prev_page)\n",
    "),\n",
    "route_list AS (\n",
    "    SELECT\n",
    "        unique_id,\n",
    "        COLLECT_LIST(event_page) AS pages\n",
    "    FROM\n",
    "        no_duplicates\n",
    "    GROUP BY\n",
    "        unique_id\n",
    "),\n",
    "route_counts AS (\n",
    "    SELECT\n",
    "        CONCAT_WS('-', pages) AS route,\n",
    "        COUNT(*) AS count\n",
    "    FROM\n",
    "        route_list\n",
    "    GROUP BY\n",
    "        route\n",
    ")\n",
    "SELECT\n",
    "    route,\n",
    "    count\n",
    "FROM\n",
    "    route_counts\n",
    "ORDER BY\n",
    "    count DESC,\n",
    "    route ASC\n",
    "LIMIT 30;\n",
    "\"\"\"\n",
    "\n",
    "# Execute the query\n",
    "result_df = spark.sql(df_single_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/21 23:17:31 WARN TaskSetManager: Stage 128 contains a task of very large size (13195 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 130:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  896|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  677|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|   main-bonus-rabota|  135|\n",
      "|    main-news-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark RDD solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_routes(line):\n",
    "    route = []\n",
    "    if len(line) == 1:\n",
    "        return line[0]\n",
    "    route.append(line[0])\n",
    "    for word in range(1, len(line)):\n",
    "        if line[word] != 'error':\n",
    "            if line[word] == route[-1]:\n",
    "                continue\n",
    "            route.append(line[word])\n",
    "        else:\n",
    "            break\n",
    "    return \"-\".join(route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/21 23:56:51 WARN TaskSetManager: Stage 337 contains a task of very large size (13226 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mapped_rdd = df.rdd.map(lambda x: [x[0], x[1], x[2], 'error', x[4]] \n",
    "                                  if 'error' in str(x[2]).lower() \n",
    "                                  else [x[0], x[1], x[2], x[3], x[4]])\n",
    "\n",
    "filtered_rdd = mapped_rdd.filter(lambda x: x[2] == 'page' or 'error' in x[2].lower())\n",
    "paired_rdd = filtered_rdd.map(lambda x: [(x[0], x[1]), [x[3]]])\n",
    "reduced_rdd = paired_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "routes_rdd = reduced_rdd.map(lambda x: (extract_routes(x[1]), 1))\n",
    "\n",
    "top_routes = routes_rdd.reduceByKey(lambda x, y: x + y)\\\n",
    "                      .sortBy(lambda x: x[1], ascending=False)\\\n",
    "                      .take(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark DF solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/21 23:28:14 WARN TaskSetManager: Stage 170 contains a task of very large size (13195 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 170:>                                                        (0 + 6) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|               route|count|\n",
      "+--------------------+-----+\n",
      "|                main| 8184|\n",
      "|        main-archive| 1113|\n",
      "|         main-rabota| 1047|\n",
      "|       main-internet|  896|\n",
      "|          main-bonus|  870|\n",
      "|           main-news|  769|\n",
      "|        main-tariffs|  677|\n",
      "|         main-online|  587|\n",
      "|          main-vklad|  518|\n",
      "| main-rabota-archive|  170|\n",
      "| main-archive-rabota|  167|\n",
      "|  main-bonus-archive|  143|\n",
      "|   main-rabota-bonus|  139|\n",
      "|   main-bonus-rabota|  135|\n",
      "|    main-news-rabota|  135|\n",
      "|main-archive-inte...|  132|\n",
      "|    main-rabota-news|  130|\n",
      "|main-internet-rabota|  129|\n",
      "|   main-archive-news|  126|\n",
      "|main-rabota-internet|  124|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"clickstream\")\n",
    "\n",
    "error_data = (\n",
    "    df\n",
    "    .filter(F.col(\"event_type\").like(\"%error%\"))\n",
    "    .groupBy(\"user_id\", \"session_id\")\n",
    "    .agg(F.min(\"timestamp\").alias(\"error_time\"))\n",
    ")\n",
    "\n",
    "unique_sessions = (\n",
    "    df\n",
    "    .withColumn(\"unique_id\", F.concat(\"user_id\", F.lit(\"_\"), \"session_id\"))\n",
    ")\n",
    "\n",
    "ordered_sessions = (\n",
    "    unique_sessions\n",
    "    .withColumn(\"row_num\", F.row_number().over(Window.partitionBy(\"unique_id\").orderBy(\"timestamp\")))\n",
    ")\n",
    "\n",
    "filtered_sessions = (\n",
    "    ordered_sessions\n",
    "    .join(error_data, (ordered_sessions.user_id == error_data.user_id) & (ordered_sessions.session_id == error_data.session_id), \"left_outer\")\n",
    "    .filter((F.col(\"error_time\").isNull()) | (ordered_sessions.timestamp < F.col(\"error_time\")))\n",
    ")\n",
    "\n",
    "distinct_pages = (\n",
    "    filtered_sessions\n",
    "    .select(\"unique_id\", \"event_page\", F.lag(\"event_page\").over(Window.partitionBy(\"unique_id\").orderBy(\"timestamp\")).alias(\"prev_page\"))\n",
    ")\n",
    "\n",
    "no_duplicates = (\n",
    "    distinct_pages\n",
    "    .filter((F.col(\"prev_page\").isNull()) | (F.col(\"event_page\") != F.col(\"prev_page\")))\n",
    "    .select(\"unique_id\", \"event_page\")\n",
    ")\n",
    "\n",
    "route_list = (\n",
    "    no_duplicates\n",
    "    .groupBy(\"unique_id\")\n",
    "    .agg(F.collect_list(\"event_page\").alias(\"pages\"))\n",
    ")\n",
    "\n",
    "route_counts = (\n",
    "    route_list\n",
    "    .groupBy(F.concat_ws(\"-\", \"pages\").alias(\"route\"))\n",
    "    .agg(F.count(\"*\").alias(\"count\"))\n",
    ")\n",
    "\n",
    "result_df = (\n",
    "    route_counts\n",
    "    .select(\"route\", \"count\")\n",
    "    .orderBy(F.desc(\"count\"), F.asc(\"route\"))\n",
    "    .limit(30)\n",
    ")\n",
    "\n",
    "# Show the result DataFrame\n",
    "result_df.show()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "week-4-spark-homework"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
